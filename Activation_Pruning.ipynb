{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6by6HisQTgavL4wS9sTxR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swalehaparvin/Model-Compression-Techniques/blob/main/Activation_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Unh0c4ifu1Yz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LAeITYXlvJq7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation pruning function\n",
        "def prune_activations(activations, pruning_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Prune activations by setting the smallest values to zero\n",
        "\n",
        "    Args:\n",
        "        activations: Tensor of activations\n",
        "        pruning_ratio: Percentage of activations to prune (0.0 to 1.0)\n",
        "\n",
        "    Returns:\n",
        "        Pruned activations tensor\n",
        "    \"\"\"\n",
        "    # Calculate threshold based on pruning ratio\n",
        "    k = int(pruning_ratio * activations.numel())\n",
        "\n",
        "    if k > 0:\n",
        "        # Get the k-th smallest value as threshold\n",
        "        threshold = torch.kthvalue(activations.flatten(), k).values\n",
        "\n",
        "        # Create mask: 1 for values above threshold, 0 for values below\n",
        "        mask = (activations > threshold).float()\n",
        "        # Apply mask to prune activations\n",
        "        pruned_activations = activations * mask\n",
        "    else:\n",
        "        pruned_activations = activations\n",
        "\n",
        "    return pruned_activations"
      ],
      "metadata": {
        "id": "vYo0gGCuwG2u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom layer with activation pruning\n",
        "class PrunedReLU(nn.Module):\n",
        "    def __init__(self, pruning_ratio=0.3):\n",
        "        super(PrunedReLU, self).__init__()\n",
        "        self.pruning_ratio = pruning_ratio\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(x)\n",
        "        x = prune_activations(x, self.pruning_ratio)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WKdcoJHkwHeP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network with activation pruning\n",
        "class PrunedNet(nn.Module):\n",
        "    def __init__(self, pruning_ratio=0.3):\n",
        "        super(PrunedNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.pruned_relu = PrunedReLU(pruning_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pruned_relu(self.fc1(x))\n",
        "        x = self.pruned_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KOEygkTxw0UY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create sample data\n",
        "    batch_size = 32\n",
        "    input_size = 784\n",
        "    x = torch.randn(batch_size, input_size)\n",
        "\n",
        "    # Test activation pruning function\n",
        "    print(\"Testing activation pruning:\")\n",
        "    activations = torch.randn(4, 4)\n",
        "    print(\"Original activations:\")\n",
        "    print(activations)\n",
        "\n",
        "    pruned = prune_activations(activations, pruning_ratio=0.5)\n",
        "    print(\"\\nPruned activations (50% pruning):\")\n",
        "    print(pruned)\n",
        "\n",
        "    # Test networks\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing networks:\")\n",
        "\n",
        "    # Regular network\n",
        "    regular_net = SimpleNet()\n",
        "    regular_output = regular_net(x)\n",
        "    print(f\"Regular network output shape: {regular_output.shape}\")\n",
        "\n",
        "    # Pruned network\n",
        "    pruned_net = PrunedNet(pruning_ratio=0.3)\n",
        "    pruned_output = pruned_net(x)\n",
        "    print(f\"Pruned network output shape: {pruned_output.shape}\")\n",
        "\n",
        "    # Count non-zero activations\n",
        "    with torch.no_grad():\n",
        "        # Get activations from first layer\n",
        "        activations_regular = F.relu(regular_net.fc1(x))\n",
        "        activations_pruned = pruned_net.pruned_relu(pruned_net.fc1(x))\n",
        "\n",
        "        non_zero_regular = torch.sum(activations_regular != 0).item()\n",
        "        non_zero_pruned = torch.sum(activations_pruned != 0).item()\n",
        "        total_activations = activations_regular.numel()\n",
        "        print(f\"\\nActivation sparsity:\")\n",
        "        print(f\"Regular network: {non_zero_regular}/{total_activations} non-zero ({non_zero_regular/total_activations*100:.1f}%)\")\n",
        "        print(f\"Pruned network: {non_zero_pruned}/{total_activations} non-zero ({non_zero_pruned/total_activations*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJodx8MGw7je",
        "outputId": "20c0a61f-7649-4763-b776-bcbeaa1c000b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing activation pruning:\n",
            "Original activations:\n",
            "tensor([[-0.5588, -0.4993, -0.6085,  1.2434],\n",
            "        [-0.5221,  0.5522, -0.3042,  0.0119],\n",
            "        [-0.2190,  0.3823,  0.2324, -0.7004],\n",
            "        [-0.6220, -2.4627,  0.7492,  0.8651]])\n",
            "\n",
            "Pruned activations (50% pruning):\n",
            "tensor([[-0.0000, -0.0000, -0.0000,  1.2434],\n",
            "        [-0.0000,  0.5522, -0.0000,  0.0119],\n",
            "        [-0.2190,  0.3823,  0.2324, -0.0000],\n",
            "        [-0.0000, -0.0000,  0.7492,  0.8651]])\n",
            "\n",
            "==================================================\n",
            "Testing networks:\n",
            "Regular network output shape: torch.Size([32, 10])\n",
            "Pruned network output shape: torch.Size([32, 10])\n",
            "\n",
            "Activation sparsity:\n",
            "Regular network: 4125/8192 non-zero (50.4%)\n",
            "Pruned network: 4050/8192 non-zero (49.4%)\n"
          ]
        }
      ]
    }
  ]
}